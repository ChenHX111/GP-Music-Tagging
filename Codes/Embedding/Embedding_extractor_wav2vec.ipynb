{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb61dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\py\\Anaconda3\\envs\\personal_CRM\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\py\\Anaconda3\\envs\\personal_CRM\\Lib\\site-packages\\musicnn\\extractor.py:7: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\py\\Anaconda3\\envs\\personal_CRM\\Lib\\site-packages\\musicnn\\models.py:5: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\py\\Anaconda3\\envs\\personal_CRM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import soundfile as sf\n",
    "from musicnn.extractor import extractor as musicnn_extractor\n",
    "\n",
    "import torch\n",
    "import resampy\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Add CLAP import\n",
    "from laion_clap import CLAP_Module\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e51c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGJamendoFeatureExtractor:\n",
    "    def __init__(self, data_path, metadata_file=\"autotagging_moodtheme.tsv\"):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor for MTG-Jamendo dataset\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to your data directory (e.g., \"E:/Oxford/Extra/ICASSP/Draft_1/mtg-jamendo-dataset/data\")\n",
    "            metadata_file (str): Name of the metadata TSV file\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.metadata_file = metadata_file\n",
    "        self.audio_base_dir = self.data_path / \"mtg-jamendo-data\"\n",
    "        self.metadata_df = None\n",
    "        self.mapping_validation = None\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        metadata_path = self.data_path / self.metadata_file\n",
    "        if not metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n",
    "\n",
    "        print(f\"Loading metadata (with tag‑fix) from: {metadata_path}\")\n",
    "\n",
    "        # Read raw lines and fix rows with extra tabs in the TAGS column\n",
    "        fixed_rows = []\n",
    "        with open(metadata_path, 'r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            header = next(reader)\n",
    "            num_cols = len(header)  # should be 6\n",
    "            for row in reader:\n",
    "                if len(row) > num_cols:\n",
    "                    # join all \"extra\" columns into the last (TAGS) field\n",
    "                    row = row[: num_cols-1 ] + ['\\t'.join(row[num_cols-1:])]\n",
    "                elif len(row) < num_cols:\n",
    "                    # optionally skip or pad\n",
    "                    row = row + ['']*(num_cols - len(row))\n",
    "                fixed_rows.append(row)\n",
    "\n",
    "        # Create DataFrame\n",
    "        self.metadata_df = pd.DataFrame(fixed_rows, columns=header)\n",
    "        print(f\"Loaded {len(self.metadata_df)} tracks from metadata\")\n",
    "        print(f\"Columns: {list(self.metadata_df.columns)}\\n\")\n",
    "        print(self.metadata_df.head(3))\n",
    "        return self.metadata_df\n",
    "    \n",
    "    def validate_audio_directory(self):\n",
    "        \"\"\"Validate the audio directory structure and check for numbered folders 00-99\"\"\"\n",
    "        if not self.audio_base_dir.exists():\n",
    "            raise FileNotFoundError(f\"Audio base directory not found: {self.audio_base_dir}\")\n",
    "        \n",
    "        # Check for numbered folders 00-99\n",
    "        numbered_dirs = []\n",
    "        for i in range(100):\n",
    "            dir_name = f\"{i:02d}\"  # Format as 00, 01, 02, ..., 99\n",
    "            dir_path = self.audio_base_dir / dir_name\n",
    "            if dir_path.exists() and dir_path.is_dir():\n",
    "                numbered_dirs.append(dir_name)\n",
    "        \n",
    "        print(f\"Found audio base directory: {self.audio_base_dir}\")\n",
    "        print(f\"Found {len(numbered_dirs)} numbered subdirectories (00-99)\")\n",
    "        \n",
    "        if len(numbered_dirs) == 0:\n",
    "            raise FileNotFoundError(\"No numbered subdirectories (00-99) found in audio directory\")\n",
    "        \n",
    "        # Count total MP3 files\n",
    "        total_mp3_count = 0\n",
    "        for dir_name in numbered_dirs[:5]:  # Sample first 5 directories\n",
    "            dir_path = self.audio_base_dir / dir_name\n",
    "            mp3_files = list(dir_path.glob(\"*.mp3\"))\n",
    "            total_mp3_count += len(mp3_files)\n",
    "            print(f\"  Directory {dir_name}: {len(mp3_files)} MP3 files\")\n",
    "        \n",
    "        print(f\"Sample shows {total_mp3_count} MP3 files in first 5 directories\")\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "    def get_audio_path(self, track_id):\n",
    "        # assume self.metadata_df is loaded\n",
    "        row = self.metadata_df.loc[self.metadata_df['TRACK_ID'] == track_id]\n",
    "        if row.empty:\n",
    "            raise KeyError(f\"No metadata found for track {track_id}\")\n",
    "        relpath = row.iloc[0]['PATH']     # e.g. \"48/948.mp3\"\n",
    "        fullpath = self.audio_base_dir / relpath\n",
    "        if not fullpath.exists():\n",
    "            raise FileNotFoundError(f\"Audio file not found at {fullpath}\")\n",
    "        return fullpath\n",
    "    \n",
    "    def validate_mapping(self, sample_size=100):\n",
    "        \"\"\"\n",
    "        Validate mapping between metadata TSV and actual audio files\n",
    "        \n",
    "        Args:\n",
    "            sample_size (int): Number of random samples to check\n",
    "        \n",
    "        Returns:\n",
    "            dict: Validation results with statistics\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"VALIDATING AUDIO-METADATA MAPPING\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        if self.metadata_df is None:\n",
    "            self.load_metadata()\n",
    "        \n",
    "        # Sample tracks for validation\n",
    "        sample_df = self.metadata_df.sample(min(sample_size, len(self.metadata_df)), random_state=42)\n",
    "        \n",
    "        validation_results = {\n",
    "            'total_checked': len(sample_df),\n",
    "            'found': 0,\n",
    "            'missing': 0,\n",
    "            'found_tracks': [],\n",
    "            'missing_tracks': [],\n",
    "            'sample_paths': []\n",
    "        }\n",
    "        \n",
    "        print(f\"Checking {len(sample_df)} random tracks from metadata...\")\n",
    "        \n",
    "        # Check if we can identify the track ID column\n",
    "        possible_id_columns = ['TRACK_ID', 'track_id', 'id', 'ID', 'TrackID']\n",
    "        track_id_col = None\n",
    "        \n",
    "        for col in possible_id_columns:\n",
    "            if col in self.metadata_df.columns:\n",
    "                track_id_col = col\n",
    "                break\n",
    "        \n",
    "        if track_id_col is None:\n",
    "            print(\"Available columns:\", list(self.metadata_df.columns))\n",
    "            raise ValueError(\"Could not identify track ID column. Please check column names.\")\n",
    "        \n",
    "        print(f\"Using '{track_id_col}' as track ID column\")\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            track_id = row[track_id_col]\n",
    "            audio_path = self.get_audio_path(track_id)\n",
    "            \n",
    "            if audio_path.exists():\n",
    "                validation_results['found'] += 1\n",
    "                validation_results['found_tracks'].append({\n",
    "                    'track_id': track_id,\n",
    "                    'path': str(audio_path),\n",
    "                    'size_mb': audio_path.stat().st_size / (1024*1024)\n",
    "                })\n",
    "                # Store first 5 found paths as samples\n",
    "                if len(validation_results['sample_paths']) < 5:\n",
    "                    validation_results['sample_paths'].append(str(audio_path))\n",
    "            else:\n",
    "                validation_results['missing'] += 1\n",
    "                validation_results['missing_tracks'].append({\n",
    "                    'track_id': track_id,\n",
    "                    'expected_path': str(audio_path)\n",
    "                })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        found_percentage = (validation_results['found'] / validation_results['total_checked']) * 100\n",
    "        \n",
    "        print(f\"\\nVALIDATION RESULTS:\")\n",
    "        print(f\"  Total checked: {validation_results['total_checked']}\")\n",
    "        print(f\"  Found: {validation_results['found']} ({found_percentage:.1f}%)\")\n",
    "        print(f\"  Missing: {validation_results['missing']} ({100-found_percentage:.1f}%)\")\n",
    "        \n",
    "        if validation_results['sample_paths']:\n",
    "            print(f\"\\nSample found audio paths:\")\n",
    "            for path in validation_results['sample_paths']:\n",
    "                print(f\"  {path}\")\n",
    "        \n",
    "        if validation_results['missing_tracks'] and len(validation_results['missing_tracks']) <= 5:\n",
    "            print(f\"\\nMissing tracks (showing first 5):\")\n",
    "            for track in validation_results['missing_tracks'][:5]:\n",
    "                print(f\"  Track {track['track_id']}: Expected at {track['expected_path']}\")\n",
    "        \n",
    "        # Check folder distribution for found tracks\n",
    "        if validation_results['found_tracks']:\n",
    "            folder_distribution = {}\n",
    "            for track in validation_results['found_tracks']:\n",
    "                folder = Path(track['path']).parent.name\n",
    "                folder_distribution[folder] = folder_distribution.get(folder, 0) + 1\n",
    "            \n",
    "            print(f\"\\nFolder distribution of found tracks:\")\n",
    "            for folder, count in sorted(folder_distribution.items()):\n",
    "                print(f\"  Folder {folder}: {count} tracks\")\n",
    "        \n",
    "        self.mapping_validation = validation_results\n",
    "        \n",
    "        if found_percentage < 50:\n",
    "            print(f\"\\n⚠️  WARNING: Only {found_percentage:.1f}% of tracks found!\")\n",
    "            print(\"This might indicate an issue with the folder structure or file naming.\")\n",
    "        else:\n",
    "            print(f\"\\n✅ Validation successful: {found_percentage:.1f}% of tracks found\")\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def load_audio_file(self, file_path, sr=22050, duration=30.0):\n",
    "        \"\"\"\n",
    "        Load an audio file using librosa\n",
    "        \n",
    "        Args:\n",
    "            file_path (Path): Path to the audio file\n",
    "            sr (int): Sample rate (default: 22050 Hz)\n",
    "            duration (float): Duration to load in seconds (default: 30.0 for 30-second clips)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (audio_data, sample_rate) or (None, None) if loading fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr_actual = librosa.load(file_path, sr=sr, duration=duration)\n",
    "            return y, sr_actual\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def process_dataset(self,\n",
    "                        feature_extractor_func,\n",
    "                        start_index: int = 0,\n",
    "                        end_index: int = None,\n",
    "                        save_interval: int = 1000,\n",
    "                        validate_first: bool = True,\n",
    "                        output_dir_name: str = \"clap_features\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main processing function to extract features from all audio files\n",
    "        \n",
    "        Args:\n",
    "            feature_extractor_func: Your feature extraction function\n",
    "            start_index (int): Starting index for processing\n",
    "            end_index (int): Ending index for processing (None for all)\n",
    "            save_interval (int): Save intermediate results every N files\n",
    "            validate_first (bool): Whether to validate mapping before processing\n",
    "            output_dir_name (str): Name of output directory for intermediate saves\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with extracted features\n",
    "        \"\"\"\n",
    "        # Load metadata and validate audio directory\n",
    "        self.load_metadata()\n",
    "        self.validate_audio_directory()\n",
    "        \n",
    "        # Run validation check first\n",
    "        if validate_first:\n",
    "            validation_results = self.validate_mapping(sample_size=50)\n",
    "            \n",
    "            # Ask user if they want to continue if validation shows issues\n",
    "            if validation_results['found'] / validation_results['total_checked'] < 0.5:\n",
    "                print(\"\\n⚠️  Low success rate in validation. Please check the issues above.\")\n",
    "                response = input(\"Do you want to continue anyway? (y/n): \")\n",
    "                if response.lower() != 'y':\n",
    "                    print(\"Processing cancelled.\")\n",
    "                    return None\n",
    "        \n",
    "        # 2) Slice the DataFrame to only the desired segment\n",
    "        total = len(self.metadata_df)\n",
    "        end_index = end_index or total\n",
    "        df_segment = self.metadata_df.iloc[start_index:end_index]  # slice with iloc\n",
    "\n",
    "        print(f\"Processing tracks {start_index} to {end_index} (total {len(df_segment)})\")\n",
    "        \n",
    "        # 2) Try to resume from the latest pickle\n",
    "        results = []\n",
    "        processed_count = start_index\n",
    "        failed_count = 0\n",
    "        last_pkl = None\n",
    "        \n",
    "        # Identify track ID column\n",
    "        possible_id_columns = ['TRACK_ID', 'track_id', 'id', 'ID', 'TrackID']\n",
    "        track_id_col = None\n",
    "        for col in possible_id_columns:\n",
    "            if col in self.metadata_df.columns:\n",
    "                track_id_col = col\n",
    "                break\n",
    "        \n",
    "        if track_id_col is None:\n",
    "            raise ValueError(f\"Could not identify track ID column. Available columns: {list(self.metadata_df.columns)}\")\n",
    "        \n",
    "        print(f\"Processing at index {start_index} (track #{processed_count+1})\")\n",
    "        print(f\"Tracks left: {len(df_segment)}\")\n",
    "        print(f\"Using '{track_id_col}' as track ID column\")\n",
    "        \n",
    "        # Create output directory for intermediate saves\n",
    "        output_dir = self.data_path / output_dir_name\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Process each track\n",
    "        for idx, row in tqdm(df_segment.iterrows(), total=len(df_segment), desc=\"Processing audio\"):\n",
    "            track_id = row[track_id_col]\n",
    "            \n",
    "            # Get audio file path\n",
    "            audio_path = self.get_audio_path(track_id)\n",
    "            \n",
    "            if not audio_path.exists():\n",
    "                # print(f\"Audio file not found: {audio_path}\")  # Comment out to reduce noise\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            audio_data, sample_rate = self.load_audio_file(audio_path)\n",
    "            \n",
    "            if audio_data is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Extract features using your feature extractor\n",
    "            try:\n",
    "                features = feature_extractor_func(audio_data, sample_rate)\n",
    "                \n",
    "                # Combine metadata with features\n",
    "                result_row = {\n",
    "                    'track_id': track_id,\n",
    "                    'audio_path': str(audio_path),\n",
    "                    'duration': len(audio_data) / sample_rate,\n",
    "                    'sample_rate': sample_rate,\n",
    "                    **features  # Unpack feature dictionary\n",
    "                }\n",
    "                \n",
    "                # Add original metadata columns\n",
    "                for col in row.index:\n",
    "                    if col not in result_row:\n",
    "                        result_row[f'meta_{col}'] = row[col]\n",
    "                \n",
    "                results.append(result_row)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Save intermediate results\n",
    "                if processed_count % save_interval == 0:\n",
    "                    temp_df = pd.DataFrame(results)\n",
    "                    # 2) Save CSV checkpoint\n",
    "                    csv_path = output_dir / f\"features_intermediate_{processed_count}.csv\"\n",
    "                    temp_df.to_csv(csv_path, index=False)  \n",
    "\n",
    "                    # 3) Save Pickle checkpoint\n",
    "                    pkl_path = output_dir / f\"features_intermediate_{processed_count}.pkl\"\n",
    "                    temp_df.to_pickle(pkl_path) \n",
    "\n",
    "                    # delete previous snapshot if any\n",
    "                    if last_pkl and os.path.exists(last_pkl):\n",
    "                        os.remove(last_pkl)      # remove old file\n",
    "                    last_pkl = str(pkl_path) \n",
    "\n",
    "                    # 4) Optional: free memory if needed\n",
    "                    del temp_df  \n",
    "                    gc.collect()\n",
    "\n",
    "                    print(f\"Saved intermediate results at {processed_count} files →\")\n",
    "                    print(f\"  • CSV:  {csv_path}\")\n",
    "                    print(f\"  • Pickle: {pkl_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Feature extraction failed for {track_id}: {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        # Create final DataFrame\n",
    "        features_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Successfully processed: {processed_count} files\")\n",
    "        print(f\"Failed: {failed_count} files\")\n",
    "        if len(features_df) > 0:\n",
    "            print(f\"Feature DataFrame shape: {features_df.shape}\")\n",
    "        \n",
    "        return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16547a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLAP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint D:/Models/630k-audioset-best.pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n",
      "CLAP model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def quick_validation_check():\n",
    "    \"\"\"\n",
    "    Quick function to just validate the mapping without doing feature extraction\n",
    "    Run this first to make sure everything is set up correctly\n",
    "    \"\"\"\n",
    "    data_path = \"E:/Oxford/Extra/ICASSP/Draft_1/mtg-jamendo-dataset/data\"\n",
    "    extractor = MTGJamendoFeatureExtractor(data_path)\n",
    "    \n",
    "    print(\"Loading metadata and validating audio directory structure...\")\n",
    "    extractor.load_metadata()\n",
    "    extractor.validate_audio_directory()\n",
    "    \n",
    "    print(\"\\nValidating audio-metadata mapping...\")\n",
    "    validation_results = extractor.validate_mapping(sample_size=100)\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def musicnn_feature_extractor(audio_data: np.ndarray,\n",
    "                              sample_rate: int,\n",
    "                              model: str = 'MTT_musicnn') -> dict:\n",
    "    \"\"\"\n",
    "    Given raw audio (numpy) + sample rate, save to temp WAV,\n",
    "    run musicnn.extractor to get intermediate representations.\n",
    "    Returns a dict with keys like 'timbral', 'temporal', 'cnn1', ...,\n",
    "    'penultimate' (for musicnn models).\n",
    "    \"\"\"\n",
    "    tmp_path = 'temp_clip.wav'\n",
    "    sf.write(tmp_path, audio_data, sample_rate)  \n",
    "    \n",
    "    # extractor returns (taggram, tags, features_dict)\n",
    "    _, _, feats = musicnn_extractor(tmp_path,\n",
    "                             model=model,\n",
    "                             extract_features=True)\n",
    "    return feats\n",
    "\n",
    "\n",
    "# Load wav2vec once at module scope:\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "model     = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "model.eval()\n",
    "\n",
    "def wav2vec_feature_extractor(audio_data: np.ndarray,\n",
    "                              sample_rate: int) -> dict:\n",
    "    \"\"\"\n",
    "    Resample to 16 kHz, tokenize with Wav2Vec2Processor,\n",
    "    run Wav2Vec2Model, and average last_hidden_state to\n",
    "    get one fixed-length embedding per clip.\n",
    "    \"\"\"\n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        audio_data = resampy.resample(audio_data, sample_rate, 16000)\n",
    "    \n",
    "    # Tokenize & run model\n",
    "    inputs = processor(audio_data,\n",
    "                       sampling_rate=16000,\n",
    "                       return_tensors='pt',\n",
    "                       padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # outputs.last_hidden_state shape (1, T, 768)\n",
    "    \n",
    "    # Mean‑pool over the time dimension → (768,)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return {'wav2vec2_embedding': embedding}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLAP Feature Extractor\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize CLAP model with local checkpoint\n",
    "print(\"Loading CLAP model...\")\n",
    "ckpt_path = \"D:/Models/630k-audioset-best.pt\"\n",
    "clap_model = CLAP_Module(enable_fusion=False)\n",
    "clap_model.load_ckpt(ckpt_path)  # loads from your local copy\n",
    "clap_model.eval()\n",
    "print(\"CLAP model loaded successfully!\")\n",
    "\n",
    "def clap_feature_extractor(audio_data: np.ndarray, sample_rate: int) -> dict:\n",
    "    \"\"\"\n",
    "    Extract CLAP embeddings from raw audio.\n",
    "    Resamples to 48 kHz, reshapes to (1, T), and pools the output.\n",
    "    \n",
    "    Args:\n",
    "        audio_data (np.ndarray): Raw audio data\n",
    "        sample_rate (int): Sample rate of the audio\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'clap_embedding' key with embedding vector\n",
    "    \"\"\"\n",
    "    # Resample to 48 kHz if needed (CLAP requires 48kHz)\n",
    "    if sample_rate != 48000:\n",
    "        audio_data = resampy.resample(audio_data, sample_rate, 48000)\n",
    "    \n",
    "    # Shape to (batch, time) - CLAP expects batch dimension\n",
    "    audio_data = audio_data.reshape(1, -1)\n",
    "    \n",
    "    # Get numpy embeddings from CLAP\n",
    "    audio_embed = clap_model.get_audio_embedding_from_data(x=audio_data, use_tensor=False)\n",
    "    \n",
    "    # Squeeze to 1D array\n",
    "    embedding = np.squeeze(audio_embed, axis=0)\n",
    "    \n",
    "    return {\"clap_embedding\": embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572f20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"E:/Oxford/Extra/ICASSP/Draft_1/mtg-jamendo-dataset/data\"\n",
    "\n",
    "DATA_ROOT = Path(\"E:/Oxford/Extra/ICASSP/Draft_1/mtg-jamendo-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97f5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) musicnn embeddings\n",
    "# extractor = MTGJamendoFeatureExtractor(data_path)\n",
    "# df_musicnn = extractor.process_dataset(\n",
    "#     feature_extractor_func=musicnn_feature_extractor,\n",
    "#     start_index=0,\n",
    "#     end_index=None,\n",
    "#     save_interval=1000,\n",
    "#     validate_first=False\n",
    "# )\n",
    "# df_musicnn.to_csv(DATA_ROOT / 'mtg_jamendo_musicnn_embeddings.csv',\n",
    "#                   index=False)\n",
    "# df_musicnn.to_pickle(DATA_ROOT / 'mtg_jamendo_musicnn_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df6e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) wav2vec2 embeddings\n",
    "# extractor = MTGJamendoFeatureExtractor(data_path)\n",
    "# df_wav2vec = extractor.process_dataset(\n",
    "#     feature_extractor_func=wav2vec_feature_extractor,\n",
    "#     start_index=0,\n",
    "#     end_index=None,\n",
    "#     save_interval=1000,\n",
    "#     validate_first=False\n",
    "# )\n",
    "# df_wav2vec.to_csv(DATA_ROOT / 'mtg_jamendo_wav2vec2_embeddings.csv',\n",
    "#                   index=False)\n",
    "# df_wav2vec.to_pickle(DATA_ROOT / 'mtg_jamendo_wav2vec2_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b7e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert wav2vec embeddings to separate columns for CSV compatibility\n",
    "# emb = df_wav2vec.pop('wav2vec2_embedding').tolist()\n",
    "# df_wav2vec[[f'w2v_{i}' for i in range(emb[0].shape[0])]] = pd.DataFrame(emb)\n",
    "\n",
    "# df_wav2vec.to_csv(DATA_ROOT / 'mtg_jamendo_wav2vec2_embeddings_v2.csv',\n",
    "#                   index=False)\n",
    "# df_wav2vec.to_pickle(DATA_ROOT / 'mtg_jamendo_wav2vec2_embeddings_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wh82dosp3hi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata (with tag‑fix) from: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\autotagging_moodtheme.tsv\n",
      "Loaded 18486 tracks from metadata\n",
      "Columns: ['TRACK_ID', 'ARTIST_ID', 'ALBUM_ID', 'PATH', 'DURATION', 'TAGS']\n",
      "\n",
      "        TRACK_ID      ARTIST_ID      ALBUM_ID        PATH DURATION  \\\n",
      "0  track_0000948  artist_000087  album_000149  48/948.mp3    212.7   \n",
      "1  track_0000950  artist_000087  album_000149  50/950.mp3    248.0   \n",
      "2  track_0000951  artist_000087  album_000149  51/951.mp3    199.7   \n",
      "\n",
      "                      TAGS  \n",
      "0  mood/theme---background  \n",
      "1  mood/theme---background  \n",
      "2  mood/theme---background  \n",
      "Found audio base directory: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\mtg-jamendo-data\n",
      "Found 100 numbered subdirectories (00-99)\n",
      "  Directory 00: 202 MP3 files\n",
      "  Directory 01: 200 MP3 files\n",
      "  Directory 02: 207 MP3 files\n",
      "  Directory 03: 190 MP3 files\n",
      "  Directory 04: 193 MP3 files\n",
      "Sample shows 992 MP3 files in first 5 directories\n",
      "Processing tracks 0 to 18486 (total 18486)\n",
      "Processing at index 0 (track #1)\n",
      "Tracks left: 18486\n",
      "Using 'TRACK_ID' as track ID column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:   5%|▌         | 1000/18486 [1:00:34<26:26:45,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 1000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_1000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_1000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  11%|█         | 2000/18486 [1:58:16<26:05:04,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 2000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_2000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_2000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  16%|█▌        | 3000/18486 [2:56:21<30:21:36,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 3000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_3000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_3000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  22%|██▏       | 4000/18486 [3:53:13<29:59:11,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 4000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_4000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_4000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  27%|██▋       | 5000/18486 [4:51:54<32:43:53,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 5000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_5000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_5000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  32%|███▏      | 6000/18486 [5:48:57<33:58:22,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 6000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_6000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_6000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  38%|███▊      | 7000/18486 [6:48:19<35:26:18, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 7000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_7000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_7000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  43%|████▎     | 8000/18486 [7:45:07<34:41:44, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 8000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_8000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_8000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  49%|████▊     | 9000/18486 [8:45:25<33:58:22, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 9000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_9000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_9000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  54%|█████▍    | 10000/18486 [9:42:49<32:35:58, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 10000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_10000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_10000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  60%|█████▉    | 11000/18486 [10:42:38<32:25:41, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 11000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_11000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_11000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  65%|██████▍   | 12000/18486 [11:41:02<28:17:40, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 12000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_12000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_12000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  70%|███████   | 13000/18486 [12:44:08<25:36:18, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 13000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_13000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_13000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  76%|███████▌  | 14000/18486 [13:42:33<25:24:31, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 14000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_14000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_14000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  81%|████████  | 15000/18486 [14:40:48<23:19:59, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 15000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_15000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_15000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  87%|████████▋ | 16000/18486 [15:38:29<15:14:39, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 16000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_16000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_16000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  92%|█████████▏| 17000/18486 [16:38:55<9:46:59, 23.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 17000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_17000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_17000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio:  97%|█████████▋| 18000/18486 [17:33:16<3:03:14, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results at 18000 files →\n",
      "  • CSV:  E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_18000.csv\n",
      "  • Pickle: E:\\Oxford\\Extra\\ICASSP\\Draft_1\\mtg-jamendo-dataset\\data\\clap_features\\features_intermediate_18000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio: 100%|██████████| 18486/18486 [17:57:33<00:00,  3.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Successfully processed: 18486 files\n",
      "Failed: 0 files\n",
      "Feature DataFrame shape: (18486, 11)\n"
     ]
    }
   ],
   "source": [
    "# 3) CLAP embeddings\n",
    "extractor = MTGJamendoFeatureExtractor(data_path)\n",
    "df_clap = extractor.process_dataset(\n",
    "    feature_extractor_func=clap_feature_extractor,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    save_interval=1000,\n",
    "    validate_first=False\n",
    ")\n",
    "df_clap.to_csv(DATA_ROOT / 'mtg_jamendo_clap_embeddings.csv',\n",
    "                  index=False)\n",
    "df_clap.to_pickle(DATA_ROOT / 'mtg_jamendo_clap_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18nbdm0814u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CLAP embeddings to separate columns for CSV compatibility\n",
    "emb = df_clap.pop('clap_embedding').tolist()\n",
    "df_clap[[f'clap_{i}' for i in range(emb[0].shape[0])]] = pd.DataFrame(emb)\n",
    "\n",
    "df_clap.to_csv(DATA_ROOT / 'mtg_jamendo_clap_embeddings_v2.csv',\n",
    "                  index=False)\n",
    "df_clap.to_pickle(DATA_ROOT / 'mtg_jamendo_clap_embeddings_v2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_CRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
