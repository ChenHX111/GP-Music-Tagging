{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import soundfile as sf\n",
    "from musicnn.extractor import extractor as musicnn_extractor\n",
    "\n",
    "import torch\n",
    "import resampy\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# Add CLAP import\n",
    "from laion_clap import CLAP_Module\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e51c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANFeatureExtractor:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor for GTZAN dataset\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to GTZAN data directory (e.g., \"E:/Oxford/Extra/ICASSP/Draft_1/GTZAN-data\")\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.audio_base_dir = self.data_path / \"genres_original\"\n",
    "        self.metadata_df = None\n",
    "        self.mapping_validation = None\n",
    "        \n",
    "        # GTZAN genre list\n",
    "        self.genres = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "                      'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"\n",
    "        Create metadata DataFrame from GTZAN folder structure\n",
    "        Each genre folder contains 100 files: genre.00000.wav to genre.00099.wav\n",
    "        \"\"\"\n",
    "        print(f\"Creating metadata from GTZAN folder structure: {self.audio_base_dir}\")\n",
    "        \n",
    "        if not self.audio_base_dir.exists():\n",
    "            raise FileNotFoundError(f\"GTZAN audio directory not found: {self.audio_base_dir}\")\n",
    "        \n",
    "        metadata_rows = []\n",
    "        \n",
    "        for genre in self.genres:\n",
    "            genre_dir = self.audio_base_dir / genre\n",
    "            if not genre_dir.exists():\n",
    "                print(f\"Warning: Genre directory not found: {genre_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # Find all .wav files in this genre directory\n",
    "            wav_files = list(genre_dir.glob(\"*.wav\"))\n",
    "            print(f\"Found {len(wav_files)} files in {genre} directory\")\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                # Extract track number from filename (e.g., blues.00042.wav -> 42)\n",
    "                filename = wav_file.stem  # e.g., \"blues.00042\"\n",
    "                parts = filename.split('.')\n",
    "                if len(parts) >= 2:\n",
    "                    track_num = parts[1]  # \"00042\"\n",
    "                else:\n",
    "                    track_num = \"unknown\"\n",
    "                \n",
    "                # Create unique track ID\n",
    "                track_id = f\"{genre}_{track_num}\"\n",
    "                \n",
    "                # Create metadata row\n",
    "                metadata_row = {\n",
    "                    'TRACK_ID': track_id,\n",
    "                    'PATH': f\"{genre}/{wav_file.name}\",  # e.g., \"blues/blues.00042.wav\"\n",
    "                    'GENRE': genre,\n",
    "                    'TRACK_NUMBER': track_num,\n",
    "                    'FILENAME': wav_file.name,\n",
    "                    'FULL_PATH': str(wav_file)\n",
    "                }\n",
    "                metadata_rows.append(metadata_row)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        self.metadata_df = pd.DataFrame(metadata_rows)\n",
    "        print(f\"\\nCreated metadata for {len(self.metadata_df)} tracks\")\n",
    "        print(f\"Genres found: {sorted(self.metadata_df['GENRE'].unique())}\")\n",
    "        print(f\"Tracks per genre: {self.metadata_df['GENRE'].value_counts().to_dict()}\")\n",
    "        print(f\"Columns: {list(self.metadata_df.columns)}\\n\")\n",
    "        print(self.metadata_df.head(3))\n",
    "        return self.metadata_df\n",
    "    \n",
    "    def validate_audio_directory(self):\n",
    "        \"\"\"Validate the GTZAN audio directory structure\"\"\"\n",
    "        if not self.audio_base_dir.exists():\n",
    "            raise FileNotFoundError(f\"Audio base directory not found: {self.audio_base_dir}\")\n",
    "        \n",
    "        print(f\"Found GTZAN audio base directory: {self.audio_base_dir}\")\n",
    "        \n",
    "        # Check for genre folders\n",
    "        found_genres = []\n",
    "        total_wav_count = 0\n",
    "        \n",
    "        for genre in self.genres:\n",
    "            genre_dir = self.audio_base_dir / genre\n",
    "            if genre_dir.exists() and genre_dir.is_dir():\n",
    "                found_genres.append(genre)\n",
    "                wav_files = list(genre_dir.glob(\"*.wav\"))\n",
    "                total_wav_count += len(wav_files)\n",
    "                print(f\"  Genre '{genre}': {len(wav_files)} WAV files\")\n",
    "            else:\n",
    "                print(f\"  Genre '{genre}': MISSING\")\n",
    "        \n",
    "        print(f\"\\nFound {len(found_genres)}/{len(self.genres)} genre directories\")\n",
    "        print(f\"Total WAV files: {total_wav_count}\")\n",
    "        \n",
    "        if len(found_genres) == 0:\n",
    "            raise FileNotFoundError(\"No genre directories found in GTZAN directory\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_audio_path(self, track_id):\n",
    "        \"\"\"Get audio file path from track ID\"\"\"\n",
    "        row = self.metadata_df.loc[self.metadata_df['TRACK_ID'] == track_id]\n",
    "        if row.empty:\n",
    "            raise KeyError(f\"No metadata found for track {track_id}\")\n",
    "        relpath = row.iloc[0]['PATH']     # e.g. \"blues/blues.00042.wav\"\n",
    "        fullpath = self.audio_base_dir / relpath\n",
    "        if not fullpath.exists():\n",
    "            raise FileNotFoundError(f\"Audio file not found at {fullpath}\")\n",
    "        return fullpath\n",
    "    \n",
    "    def validate_mapping(self, sample_size=100):\n",
    "        \"\"\"\n",
    "        Validate mapping between metadata and actual audio files\n",
    "        \n",
    "        Args:\n",
    "            sample_size (int): Number of random samples to check\n",
    "        \n",
    "        Returns:\n",
    "            dict: Validation results with statistics\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"VALIDATING AUDIO-METADATA MAPPING\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        if self.metadata_df is None:\n",
    "            self.load_metadata()\n",
    "        \n",
    "        # Sample tracks for validation\n",
    "        sample_df = self.metadata_df.sample(min(sample_size, len(self.metadata_df)), random_state=42)\n",
    "        \n",
    "        validation_results = {\n",
    "            'total_checked': len(sample_df),\n",
    "            'found': 0,\n",
    "            'missing': 0,\n",
    "            'found_tracks': [],\n",
    "            'missing_tracks': [],\n",
    "            'sample_paths': []\n",
    "        }\n",
    "        \n",
    "        print(f\"Checking {len(sample_df)} random tracks from metadata...\")\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            track_id = row['TRACK_ID']\n",
    "            audio_path = self.get_audio_path(track_id)\n",
    "            \n",
    "            if audio_path.exists():\n",
    "                validation_results['found'] += 1\n",
    "                validation_results['found_tracks'].append({\n",
    "                    'track_id': track_id,\n",
    "                    'path': str(audio_path),\n",
    "                    'size_mb': audio_path.stat().st_size / (1024*1024),\n",
    "                    'genre': row['GENRE']\n",
    "                })\n",
    "                # Store first 5 found paths as samples\n",
    "                if len(validation_results['sample_paths']) < 5:\n",
    "                    validation_results['sample_paths'].append(str(audio_path))\n",
    "            else:\n",
    "                validation_results['missing'] += 1\n",
    "                validation_results['missing_tracks'].append({\n",
    "                    'track_id': track_id,\n",
    "                    'expected_path': str(audio_path)\n",
    "                })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        found_percentage = (validation_results['found'] / validation_results['total_checked']) * 100\n",
    "        \n",
    "        print(f\"\\nVALIDATION RESULTS:\")\n",
    "        print(f\"  Total checked: {validation_results['total_checked']}\")\n",
    "        print(f\"  Found: {validation_results['found']} ({found_percentage:.1f}%)\")\n",
    "        print(f\"  Missing: {validation_results['missing']} ({100-found_percentage:.1f}%)\")\n",
    "        \n",
    "        if validation_results['sample_paths']:\n",
    "            print(f\"\\nSample found audio paths:\")\n",
    "            for path in validation_results['sample_paths']:\n",
    "                print(f\"  {path}\")\n",
    "        \n",
    "        if validation_results['missing_tracks'] and len(validation_results['missing_tracks']) <= 5:\n",
    "            print(f\"\\nMissing tracks (showing first 5):\")\n",
    "            for track in validation_results['missing_tracks'][:5]:\n",
    "                print(f\"  Track {track['track_id']}: Expected at {track['expected_path']}\")\n",
    "        \n",
    "        # Check genre distribution for found tracks\n",
    "        if validation_results['found_tracks']:\n",
    "            genre_distribution = {}\n",
    "            for track in validation_results['found_tracks']:\n",
    "                genre = track['genre']\n",
    "                genre_distribution[genre] = genre_distribution.get(genre, 0) + 1\n",
    "            \n",
    "            print(f\"\\nGenre distribution of found tracks:\")\n",
    "            for genre, count in sorted(genre_distribution.items()):\n",
    "                print(f\"  {genre}: {count} tracks\")\n",
    "        \n",
    "        self.mapping_validation = validation_results\n",
    "        \n",
    "        if found_percentage < 50:\n",
    "            print(f\"\\n⚠️  WARNING: Only {found_percentage:.1f}% of tracks found!\")\n",
    "            print(\"This might indicate an issue with the folder structure or file naming.\")\n",
    "        else:\n",
    "            print(f\"\\n✅ Validation successful: {found_percentage:.1f}% of tracks found\")\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def load_audio_file(self, file_path, sr=22050, duration=30.0):\n",
    "        \"\"\"\n",
    "        Load an audio file using librosa\n",
    "        \n",
    "        Args:\n",
    "            file_path (Path): Path to the audio file\n",
    "            sr (int): Sample rate (default: 22050 Hz)\n",
    "            duration (float): Duration to load in seconds (default: 30.0 for 30-second clips)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (audio_data, sample_rate) or (None, None) if loading fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr_actual = librosa.load(file_path, sr=sr, duration=duration)\n",
    "            return y, sr_actual\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def process_dataset(self,\n",
    "                        feature_extractor_func,\n",
    "                        start_index: int = 0,\n",
    "                        end_index: int = None,\n",
    "                        save_interval: int = 100,\n",
    "                        validate_first: bool = True,\n",
    "                        output_dir_name: str = \"clap_features\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main processing function to extract features from all audio files\n",
    "        \n",
    "        Args:\n",
    "            feature_extractor_func: Your feature extraction function\n",
    "            start_index (int): Starting index for processing\n",
    "            end_index (int): Ending index for processing (None for all)\n",
    "            save_interval (int): Save intermediate results every N files\n",
    "            validate_first (bool): Whether to validate mapping before processing\n",
    "            output_dir_name (str): Name of output directory for intermediate saves\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with extracted features\n",
    "        \"\"\"\n",
    "        # Load metadata and validate audio directory\n",
    "        self.load_metadata()\n",
    "        self.validate_audio_directory()\n",
    "        \n",
    "        # Run validation check first\n",
    "        if validate_first:\n",
    "            validation_results = self.validate_mapping(sample_size=50)\n",
    "            \n",
    "            # Ask user if they want to continue if validation shows issues\n",
    "            if validation_results['found'] / validation_results['total_checked'] < 0.5:\n",
    "                print(\"\\n⚠️  Low success rate in validation. Please check the issues above.\")\n",
    "                response = input(\"Do you want to continue anyway? (y/n): \")\n",
    "                if response.lower() != 'y':\n",
    "                    print(\"Processing cancelled.\")\n",
    "                    return None\n",
    "        \n",
    "        # Slice the DataFrame to only the desired segment\n",
    "        total = len(self.metadata_df)\n",
    "        end_index = end_index or total\n",
    "        df_segment = self.metadata_df.iloc[start_index:end_index]\n",
    "\n",
    "        print(f\"Processing tracks {start_index} to {end_index} (total {len(df_segment)})\")\n",
    "        \n",
    "        # Initialize processing variables\n",
    "        results = []\n",
    "        processed_count = start_index\n",
    "        failed_count = 0\n",
    "        last_pkl = None\n",
    "        \n",
    "        print(f\"Processing starting at index {start_index} (track #{processed_count+1})\")\n",
    "        print(f\"Tracks left: {len(df_segment)}\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = self.data_path / output_dir_name\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Process each track\n",
    "        for idx, row in tqdm(df_segment.iterrows(), total=len(df_segment), desc=\"Processing audio\"):\n",
    "            track_id = row['TRACK_ID']\n",
    "            \n",
    "            # Get audio file path\n",
    "            audio_path = self.get_audio_path(track_id)\n",
    "            \n",
    "            if not audio_path.exists():\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Load audio\n",
    "            audio_data, sample_rate = self.load_audio_file(audio_path)\n",
    "            \n",
    "            if audio_data is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Extract features using your feature extractor\n",
    "            try:\n",
    "                features = feature_extractor_func(audio_data, sample_rate)\n",
    "                \n",
    "                # Combine metadata with features\n",
    "                result_row = {\n",
    "                    'track_id': track_id,\n",
    "                    'audio_path': str(audio_path),\n",
    "                    'duration': len(audio_data) / sample_rate,\n",
    "                    'sample_rate': sample_rate,\n",
    "                    **features  # Unpack feature dictionary\n",
    "                }\n",
    "                \n",
    "                # Add original metadata columns\n",
    "                for col in row.index:\n",
    "                    if col not in result_row:\n",
    "                        result_row[f'meta_{col}'] = row[col]\n",
    "                \n",
    "                results.append(result_row)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Save intermediate results\n",
    "                if processed_count % save_interval == 0:\n",
    "                    temp_df = pd.DataFrame(results)\n",
    "                    \n",
    "                    # Save CSV checkpoint\n",
    "                    csv_path = output_dir / f\"gtzan_features_intermediate_{processed_count}.csv\"\n",
    "                    temp_df.to_csv(csv_path, index=False)  \n",
    "\n",
    "                    # Save Pickle checkpoint\n",
    "                    pkl_path = output_dir / f\"gtzan_features_intermediate_{processed_count}.pkl\"\n",
    "                    temp_df.to_pickle(pkl_path) \n",
    "\n",
    "                    # Delete previous snapshot if any\n",
    "                    if last_pkl and os.path.exists(last_pkl):\n",
    "                        os.remove(last_pkl)\n",
    "                    last_pkl = str(pkl_path) \n",
    "\n",
    "                    # Optional: free memory if needed\n",
    "                    del temp_df  \n",
    "                    gc.collect()\n",
    "\n",
    "                    print(f\"Saved intermediate results at {processed_count} files →\")\n",
    "                    print(f\"  • CSV:  {csv_path}\")\n",
    "                    print(f\"  • Pickle: {pkl_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Feature extraction failed for {track_id}: {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        # Create final DataFrame\n",
    "        features_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Successfully processed: {processed_count} files\")\n",
    "        print(f\"Failed: {failed_count} files\")\n",
    "        if len(features_df) > 0:\n",
    "            print(f\"Feature DataFrame shape: {features_df.shape}\")\n",
    "        \n",
    "        return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16547a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_validation_check():\n",
    "    \"\"\"\n",
    "    Quick function to just validate the mapping without doing feature extraction\n",
    "    Run this first to make sure everything is set up correctly\n",
    "    \"\"\"\n",
    "    data_path = \"E:/Oxford/Extra/ICASSP/Draft_1/GTZAN-data\"\n",
    "    extractor = GTZANFeatureExtractor(data_path)\n",
    "    \n",
    "    print(\"Loading metadata and validating audio directory structure...\")\n",
    "    extractor.load_metadata()\n",
    "    extractor.validate_audio_directory()\n",
    "    \n",
    "    print(\"\\nValidating audio-metadata mapping...\")\n",
    "    validation_results = extractor.validate_mapping(sample_size=100)\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def musicnn_feature_extractor(audio_data: np.ndarray,\n",
    "                              sample_rate: int,\n",
    "                              model: str = 'MTT_musicnn') -> dict:\n",
    "    \"\"\"\n",
    "    Given raw audio (numpy) + sample rate, save to temp WAV,\n",
    "    run musicnn.extractor to get intermediate representations.\n",
    "    Returns a dict with keys like 'timbral', 'temporal', 'cnn1', ...,\n",
    "    'penultimate' (for musicnn models).\n",
    "    \"\"\"\n",
    "    tmp_path = 'temp_clip.wav'\n",
    "    sf.write(tmp_path, audio_data, sample_rate)  \n",
    "    \n",
    "    # extractor returns (taggram, tags, features_dict)\n",
    "    _, _, feats = musicnn_extractor(tmp_path,\n",
    "                             model=model,\n",
    "                             extract_features=True)\n",
    "    return feats\n",
    "\n",
    "\n",
    "# Load wav2vec once at module scope:\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "model     = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "model.eval()\n",
    "\n",
    "def wav2vec_feature_extractor(audio_data: np.ndarray,\n",
    "                              sample_rate: int) -> dict:\n",
    "    \"\"\"\n",
    "    Resample to 16 kHz, tokenize with Wav2Vec2Processor,\n",
    "    run Wav2Vec2Model, and average last_hidden_state to\n",
    "    get one fixed-length embedding per clip.\n",
    "    \"\"\"\n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        audio_data = resampy.resample(audio_data, sample_rate, 16000)\n",
    "    \n",
    "    # Tokenize & run model\n",
    "    inputs = processor(audio_data,\n",
    "                       sampling_rate=16000,\n",
    "                       return_tensors='pt',\n",
    "                       padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # outputs.last_hidden_state shape (1, T, 768)\n",
    "    \n",
    "    # Mean‑pool over the time dimension → (768,)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return {'wav2vec2_embedding': embedding}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLAP Feature Extractor\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize CLAP model with local checkpoint\n",
    "print(\"Loading CLAP model...\")\n",
    "ckpt_path = \"D:/Models/630k-audioset-best.pt\"\n",
    "clap_model = CLAP_Module(enable_fusion=False)\n",
    "clap_model.load_ckpt(ckpt_path)  # loads from your local copy\n",
    "clap_model.eval()\n",
    "print(\"CLAP model loaded successfully!\")\n",
    "\n",
    "def clap_feature_extractor(audio_data: np.ndarray, sample_rate: int) -> dict:\n",
    "    \"\"\"\n",
    "    Extract CLAP embeddings from raw audio.\n",
    "    Resamples to 48 kHz, reshapes to (1, T), and pools the output.\n",
    "    \n",
    "    Args:\n",
    "        audio_data (np.ndarray): Raw audio data\n",
    "        sample_rate (int): Sample rate of the audio\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'clap_embedding' key with embedding vector\n",
    "    \"\"\"\n",
    "    # Resample to 48 kHz if needed (CLAP requires 48kHz)\n",
    "    if sample_rate != 48000:\n",
    "        audio_data = resampy.resample(audio_data, sample_rate, 48000)\n",
    "    \n",
    "    # Shape to (batch, time) - CLAP expects batch dimension\n",
    "    audio_data = audio_data.reshape(1, -1)\n",
    "    \n",
    "    # Get numpy embeddings from CLAP\n",
    "    audio_embed = clap_model.get_audio_embedding_from_data(x=audio_data, use_tensor=False)\n",
    "    \n",
    "    # Squeeze to 1D array\n",
    "    embedding = np.squeeze(audio_embed, axis=0)\n",
    "    \n",
    "    return {\"clap_embedding\": embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"E:/Oxford/Extra/ICASSP/Draft_1/GTZAN-data\"\n",
    "\n",
    "DATA_ROOT = Path(\"E:/Oxford/Extra/ICASSP/Draft_1/GTZAN-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation check to ensure everything is set up correctly\n",
    "validation_results = quick_validation_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) musicnn embeddings\n",
    "# extractor = GTZANFeatureExtractor(data_path)\n",
    "# df_musicnn = extractor.process_dataset(\n",
    "#     feature_extractor_func=musicnn_feature_extractor,\n",
    "#     start_index=0,\n",
    "#     end_index=None,\n",
    "#     save_interval=100,\n",
    "#     validate_first=False\n",
    "# )\n",
    "# df_musicnn.to_csv(DATA_ROOT / 'gtzan_musicnn_embeddings.csv',\n",
    "#                   index=False)\n",
    "# df_musicnn.to_pickle(DATA_ROOT / 'gtzan_musicnn_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) wav2vec2 embeddings\n",
    "# extractor = GTZANFeatureExtractor(data_path)\n",
    "# df_wav2vec = extractor.process_dataset(\n",
    "#     feature_extractor_func=wav2vec_feature_extractor,\n",
    "#     start_index=0,\n",
    "#     end_index=None,\n",
    "#     save_interval=100,\n",
    "#     validate_first=False\n",
    "# )\n",
    "# df_wav2vec.to_csv(DATA_ROOT / 'gtzan_wav2vec2_embeddings.csv',\n",
    "#                   index=False)\n",
    "# df_wav2vec.to_pickle(DATA_ROOT / 'gtzan_wav2vec2_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert wav2vec embeddings to separate columns for CSV compatibility\n",
    "# emb = df_wav2vec.pop('wav2vec2_embedding').tolist()\n",
    "# df_wav2vec[[f'w2v_{i}' for i in range(len(emb[0]))]] = pd.DataFrame(emb)\n",
    "\n",
    "# df_wav2vec.to_csv(DATA_ROOT / 'gtzan_wav2vec2_embeddings_v2.csv',\n",
    "#                   index=False)\n",
    "# df_wav2vec.to_pickle(DATA_ROOT / 'gtzan_wav2vec2_embeddings_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) CLAP embeddings\n",
    "extractor = GTZANFeatureExtractor(data_path)\n",
    "df_clap = extractor.process_dataset(\n",
    "    feature_extractor_func=clap_feature_extractor,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    save_interval=300,\n",
    "    validate_first=False\n",
    ")\n",
    "df_clap.to_csv(DATA_ROOT / 'gtzan_clap_embeddings.csv',\n",
    "                  index=False)\n",
    "df_clap.to_pickle(DATA_ROOT / 'gtzan_clap_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CLAP embeddings to separate columns for CSV compatibility\n",
    "emb = df_clap.pop('clap_embedding').tolist()\n",
    "df_clap[[f'clap_{i}' for i in range(len(emb[0]))]] = pd.DataFrame(emb)\n",
    "\n",
    "df_clap.to_csv(DATA_ROOT / 'gtzan_clap_embeddings_v2.csv',\n",
    "                  index=False)\n",
    "df_clap.to_pickle(DATA_ROOT / 'gtzan_clap_embeddings_v2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_CRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
